@rifaterdemsahin ➜ /workspaces/SonarQube/6_Symbols/2_minikube (main) $ kubectl port-forward -n sonarqube svc/sonarqube 9000:9000 
Forwarding from 127.0.0.1:9000 -> 9000
Forwarding from [::1]:9000 -> 9000
Handling connection for 9000
E0404 20:18:02.002081   49163 portforward.go:424] "Unhandled Error" err=<
        an error occurred forwarding 9000 -> 9000: error forwarding port 9000 to pod c379d59ab5f99f56484f6d20fc67381df9870d995fc7295b17c0dd1065e55e1a, uid : exit status 1: 2025/04/04 20:18:01 socat[6985] E connect(5, AF=2 127.0.0.1:9000, 16): Connection refused
 >
error: lost connection to pod
@rifaterdemsahin ➜ /workspaces/SonarQube/6_Symbols/2_minikube (main) $ kubectl get all -n sonarqube
NAME                                READY   STATUS             RESTARTS      AGE
pod/sonarqube-7dcc6f468-jnd25       0/1     CrashLoopBackOff   4 (79s ago)   4m36s
pod/sonarqube-db-544499bf77-cmld6   1/1     Running            0             6m38s

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/sonarqube      NodePort    10.99.51.108    <none>        9000:30900/TCP   3m49s
service/sonarqube-db   ClusterIP   10.107.37.194   <none>        5432/TCP         6m38s

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/sonarqube      0/1     1            0           4m36s
deployment.apps/sonarqube-db   1/1     1            1           6m38s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/sonarqube-7dcc6f468       1         1         0       4m36s
replicaset.apps/sonarqube-db-544499bf77   1         1         1       6m38s
@rifaterdemsahin ➜ /workspaces/SonarQube/6_Symbols/2_minikube (main) $ 

/fix 

--

Based on the output you've provided, I can see that the SonarQube pod is in a `CrashLoopBackOff` state, which is why the port forwarding is failing. Here's how to fix this issue:

### Steps to Fix:

1. **Check the SonarQube pod logs** to understand why it's crashing:
   ```bash
   kubectl logs -n sonarqube pod/sonarqube-7dcc6f468-jnd25
   ```

2. **Common causes for SonarQube CrashLoopBackOff**:
   - Database connection issues (SonarQube can't connect to PostgreSQL)
   - Insufficient resources (CPU/Memory)
   - Incorrect configuration

3. **Check database connectivity**:
   Since your `sonarqube-db` pod is running, verify if SonarQube is properly configured to connect to it. The connection string should point to `sonarqube-db` service.

4. **Increase resources** (if needed):
   SonarQube typically requires at least 2GB RAM. If your Minikube doesn't have enough resources, you can allocate more:
   ```bash
   minikube stop
   minikube start --memory=4096 --cpus=2
   ```

5. **Check PVC/PV** (if using persistent storage):
   ```bash
   kubectl get pvc -n sonarqube
   kubectl get pv
   ```

6. **Restart the deployment**:
   ```bash
   kubectl rollout restart deployment -n sonarqube sonarqube
   ```

7. **Alternative access method**:
   Since you have a NodePort service (30900), you can access SonarQube directly via:
   ```bash
   minikube service -n sonarqube sonarqube
   ```

### If you need to debug further:

- Describe the pod to get more details:
  ```bash
  kubectl describe pod -n sonarqube sonarqube-7dcc6f468-jnd25
  ```

- Check if the database is initialized properly:
  ```bash
  kubectl exec -n sonarqube -it pod/sonarqube-db-544499bf77-cmld6 -- psql -U sonar -W
  ```

The most likely issue is either database connectivity or insufficient resources. The logs from step 1 will give you the exact reason for the crashes.


---

@rifaterdemsahin ➜ /workspaces/SonarQube/6_Symbols/2_minikube (main) $ kubectl logs -n sonarqube pod/sonarqube-7dcc6f468-jnd25
2025.04.04 20:19:15 INFO  app[][o.s.a.AppFileSystem] Cleaning or creating temp directory /opt/sonarqube/temp
2025.04.04 20:19:15 INFO  app[][o.s.a.es.EsSettings] Elasticsearch listening on [HTTP: 127.0.0.1:9001, TCP: 127.0.0.1:37991]
2025.04.04 20:19:15 INFO  app[][o.s.a.ProcessLauncherImpl] Launch process[ELASTICSEARCH] from [/opt/sonarqube/elasticsearch]: /opt/sonarqube/elasticsearch/bin/elasticsearch
2025.04.04 20:19:15 INFO  app[][o.s.a.SchedulerImpl] Waiting for Elasticsearch to be up and running
2025.04.04 20:19:20 INFO  es[][o.e.n.Node] version[7.17.15], pid[20], build[default/tar/0b8ecfb4378335f4689c4223d1f1115f16bef3ba/2023-11-10T22:03:46.987399016Z], OS[Linux/6.8.0-1021-azure/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.14/17.0.14+7]
2025.04.04 20:19:20 INFO  es[][o.e.n.Node] JVM home [/opt/java/openjdk]
2025.04.04 20:19:20 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=/opt/sonarqube/logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Des.enforce.bootstrap.checks=true, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] loaded module [reindex]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2025.04.04 20:19:21 INFO  es[][o.e.p.PluginsService] no plugins loaded
2025.04.04 20:19:21 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/ (overlay)]], net usable_space [8.6gb], net total_space [31.3gb], types [overlay]
2025.04.04 20:19:21 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2025.04.04 20:19:21 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [AeoledSCSGmkbINPvbbi1Q], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2025.04.04 20:19:29 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2025.04.04 20:19:29 INFO  es[][o.e.i.r.RecoverySettings] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
2025.04.04 20:19:29 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2025.04.04 20:19:29 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] initialized
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] starting ...
2025.04.04 20:19:30 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:37991}, bound_addresses {127.0.0.1:37991}
2025.04.04 20:19:30 INFO  es[][o.e.b.BootstrapChecks] explicitly enforcing bootstrap checks

ERROR: [1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
ERROR: Elasticsearch did not exit normally - check the logs at /opt/sonarqube/logs/sonarqube.log
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] stopping ...
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] stopped
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] closing ...
2025.04.04 20:19:30 INFO  es[][o.e.n.Node] closed
2025.04.04 20:19:31 WARN  app[][o.s.a.p.AbstractManagedProcess] Process exited with exit value [ElasticSearch]: 78
2025.04.04 20:19:31 INFO  app[][o.s.a.SchedulerImpl] Process[ElasticSearch] is stopped
2025.04.04 20:19:31 INFO  app[][o.s.a.SchedulerImpl] SonarQube is stopped
@rifaterdemsahin ➜ /workspaces/SonarQube/6_Symbols/2_minikube (main) $ 

---

The error is clear: **Elasticsearch is failing to start because the `vm.max_map_count` setting is too low** on your Minikube node. This is a common requirement for Elasticsearch to operate properly.

### Solution:

1. **Increase `vm.max_map_count` in Minikube**:

   You need to SSH into your Minikube node and increase this kernel parameter:

   ```bash
   minikube ssh
   sudo sysctl -w vm.max_map_count=262144
   exit
   ```

2. **Restart SonarQube deployment**:

   After applying the change, restart the SonarQube pod:

   ```bash
   kubectl rollout restart deployment -n sonarqube sonarqube
   ```

3. **Verify the pod comes up**:

   Check the status:
   ```bash
   kubectl get pods -n sonarqube
   ```

4. **(Optional) Make the change persistent**:

   If you restart Minikube, you'll need to set this again. To make it persistent:
   ```bash
   minikube ssh "echo 'vm.max_map_count=262144' | sudo tee -a /etc/sysctl.conf"
   ```

### Why This Works:
- Elasticsearch requires a higher `vm.max_map_count` (memory mapping areas) than the default Linux setting
- The error explicitly states it needs at least 262144 (your current is 65530)
- This is a host system setting, not something you configure in Kubernetes manifests

### Alternative (if still having issues):
If SonarQube still fails after this, check:
- If you have enough memory allocated to Minikube (recommend at least 4GB)
- Database connectivity between SonarQube and PostgreSQL
- Storage permissions if using persistent volumes

The logs show Elasticsearch is the component failing, and this configuration change should resolve that specific issue.
